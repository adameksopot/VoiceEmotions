# -*- coding: utf-8 -*-
"""VoiceRecognitionMLPModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IzCrDIId1PEm0JYWn9007lyksgdalwYl
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
import os
import pickle

data = pd.read_csv('mfcc13_df.csv', index_col=False)
labels = data.iloc[:,[-1]]
data.describe()

data.info()

data.isnull().sum()

data.isna().sum()
data.loc[0][:]

"""##Outliers

"""

Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
print(IQR)
print(data.any() < (Q1 - 1.5*IQR)) and (data.any() > (Q3 + 1.5*IQR))
#from scipy import stats
#data=data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]
data = data.drop(labels.columns,axis = 1) # dropping labels column

for column in data:
    plt.figure()
    data.boxplot([column])

"""##Standarization"""

data +=848.919070
x = data.values
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
data = pd.DataFrame(x_scaled)

data.describe()

"""##Splitting data

"""

X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=50)

print("Number of training samples:", X_train.shape[0])

print("Number of testing samples:", X_test.shape[0])

print("Number of features:", X_train.shape[1])

"""## Grid Search - finding best combination of hyperparameters"""

model = MLPClassifier()

parameter_space = {
    'hidden_layer_sizes': [(200,),(300,)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.01, 0.1],
    'learning_rate': ['constant','adaptive'],
    'max_iter': [200, 500],
    'batch_size': [100, 254]
    
}

clf = GridSearchCV(model, parameter_space, n_jobs=-1, cv=5)
clf.fit(X_train, y_train)

print('Best parameters found:\n', clf.best_params_)

model_params = {
    'activation': 'tanh',
    'alpha': 0.01,
    'batch_size': 254,
    'hidden_layer_sizes': (300,),
    'learning_rate': 'adaptive',
    'max_iter': 200,
    'solver': 'adam'
}

means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

#model = MLPClassifier(**model_paramss)
#model.fit(X_p_train, y_p_train)

y_true, y_pred = y_test, clf.predict(X_test)

accuracy = accuracy_score(y_true, y_pred)

print("Accuracy: {:.2f}%".format(accuracy*100))

print('Results on the test set:')
print(classification_report(y_true, y_pred))
confusion_matrix(y_true, y_pred)

"""## Saving model"""

if not os.path.isdir("result"):
    os.mkdir("result")

pickle.dump(clf, open("result/mlp_classifier.model", "wb"))